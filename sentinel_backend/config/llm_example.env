# LLM Provider Configuration Example
# This file shows how to configure different LLM providers for the Sentinel platform

# ====================
# Primary LLM Provider
# ====================
# Options: openai, anthropic, google, mistral, ollama, vllm, none
# DEFAULT: Anthropic Claude Sonnet 4 (balanced performance and cost)
SENTINEL_APP_LLM_PROVIDER=anthropic
SENTINEL_APP_LLM_MODEL=claude-sonnet-4

# LLM Generation Parameters
SENTINEL_APP_LLM_TEMPERATURE=0.7
SENTINEL_APP_LLM_MAX_TOKENS=2000
SENTINEL_APP_LLM_TOP_P=1.0
SENTINEL_APP_LLM_TIMEOUT=60
SENTINEL_APP_LLM_MAX_RETRIES=3

# ====================
# Provider API Keys
# ====================
# OpenAI
SENTINEL_APP_OPENAI_API_KEY=sk-...your-openai-key...

# Anthropic (Claude)
SENTINEL_APP_ANTHROPIC_API_KEY=sk-ant-...your-anthropic-key...

# Google (Gemini)
SENTINEL_APP_GOOGLE_API_KEY=...your-google-key...

# Mistral
SENTINEL_APP_MISTRAL_API_KEY=...your-mistral-key...

# ====================
# Local Model Providers
# ====================
# Ollama (for open-source models)
SENTINEL_APP_OLLAMA_BASE_URL=http://localhost:11434

# vLLM (for high-performance serving)
SENTINEL_APP_VLLM_BASE_URL=http://localhost:8000

# ====================
# Fallback Configuration
# ====================
# Enable automatic fallback to secondary providers
SENTINEL_APP_LLM_FALLBACK_ENABLED=true

# Fallback provider priority order (Anthropic first, then OpenAI, then local)
SENTINEL_APP_LLM_FALLBACK_PROVIDERS=["anthropic", "openai", "ollama"]

# Fallback models per provider
SENTINEL_APP_LLM_FALLBACK_MODELS='{"anthropic": "claude-sonnet-4", "openai": "gpt-3.5-turbo", "google": "gemini-2.5-flash", "mistral": "mistral-small-3", "ollama": "mistral:7b"}'

# ====================
# Cost Management
# ====================
# Enable cost tracking for commercial APIs
SENTINEL_APP_LLM_COST_TRACKING_ENABLED=true

# Alert threshold in USD
SENTINEL_APP_LLM_COST_ALERT_THRESHOLD=10.0

# ====================
# Caching
# ====================
# Enable response caching to reduce API calls
SENTINEL_APP_LLM_CACHE_ENABLED=true

# Cache time-to-live in seconds
SENTINEL_APP_LLM_CACHE_TTL=3600

# ====================
# Optimization
# ====================
# Enable model-specific prompt optimization
SENTINEL_APP_LLM_ENABLE_PROMPT_OPTIMIZATION=true

# Enable streaming responses (where supported)
SENTINEL_APP_LLM_ENABLE_STREAMING=false

# ====================
# Example Configurations
# ====================

# --- Example 1: Anthropic Claude Sonnet (DEFAULT - Recommended) ---
# SENTINEL_APP_LLM_PROVIDER=anthropic
# SENTINEL_APP_LLM_MODEL=claude-sonnet-4
# SENTINEL_APP_ANTHROPIC_API_KEY=sk-ant-...

# --- Example 2: Anthropic Claude Opus (Higher performance, higher cost) ---
# SENTINEL_APP_LLM_PROVIDER=anthropic
# SENTINEL_APP_LLM_MODEL=claude-opus-4.1
# SENTINEL_APP_ANTHROPIC_API_KEY=sk-ant-...

# --- Example 3: OpenAI GPT-4 ---
# SENTINEL_APP_LLM_PROVIDER=openai
# SENTINEL_APP_LLM_MODEL=gpt-4-turbo
# SENTINEL_APP_OPENAI_API_KEY=sk-...

# --- Example 4: Local Ollama with DeepSeek ---
# SENTINEL_APP_LLM_PROVIDER=ollama
# SENTINEL_APP_LLM_MODEL=deepseek-r1:70b
# SENTINEL_APP_OLLAMA_BASE_URL=http://localhost:11434

# --- Example 5: Google Gemini 2.5 Pro ---
# SENTINEL_APP_LLM_PROVIDER=google
# SENTINEL_APP_LLM_MODEL=gemini-2.5-pro
# SENTINEL_APP_GOOGLE_API_KEY=...

# --- Example 6: Disable LLM (deterministic only) ---
# SENTINEL_APP_LLM_PROVIDER=none